{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "industrial-charger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor policy_evaluation, policy_improvement, policy_iteration and value_iteration,\\nthe parameters P, nS, nA, gamma are defined as follows:\\n\\n\\tP: nested dictionary\\n\\t\\tFrom gym.core.Environment\\n\\t\\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\\n\\t\\ttuple of the form (probability, nextstate, reward, terminal) where\\n\\t\\t\\t- probability: float\\n\\t\\t\\t\\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\\n\\t\\t\\t- nextstate: int\\n\\t\\t\\t\\tdenotes the state we transition to (in range [0, nS - 1])\\n\\t\\t\\t- reward: int\\n\\t\\t\\t\\teither 0 or 1, the reward for transitioning from \"state\" to\\n\\t\\t\\t\\t\"nextstate\" with \"action\"\\n\\t\\t\\t- terminal: bool\\n\\t\\t\\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\\n\\tnS: int\\n\\t\\tnumber of states in the environment\\n\\tnA: int\\n\\t\\tnumber of actions in the environment\\n\\tgamma: float\\n\\t\\tDiscount factor. Number in range [0, 1)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MDP Value Iteration and Policy Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "\tP: nested dictionary\n",
    "\t\tFrom gym.core.Environment\n",
    "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "spatial-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True to turn-on debug messaging\n",
    "PRINT_DEBUG = True\n",
    "def printDbgMsg(msg):\n",
    "    if PRINT_DEBUG:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demanding-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tpolicy: np.array[nS]\n",
    "\t\tThe policy to evaluate. Maps states to actions.\n",
    "\ttol: float\n",
    "\t\tTerminate policy evaluation when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns\n",
    "\t-------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\t\tThe value function of the given policy, where value_function[s] is\n",
    "\t\tthe value of state s\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "\n",
    "\t############################\n",
    "\treturn value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cheap-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\t\"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tvalue_from_policy: np.ndarray\n",
    "\t\tThe value calculated from the policy\n",
    "\tpolicy: np.array\n",
    "\t\tThe previous policy.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnew_policy: np.ndarray[nS]\n",
    "\t\tAn array of integers. Each integer is the optimal action to take\n",
    "\t\tin that state according to the environment dynamics and the\n",
    "\t\tgiven value function.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnew_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "\n",
    "\t############################\n",
    "\treturn new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cosmetic-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "\t\"\"\"Runs policy iteration.\n",
    "\n",
    "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
    "\timplement this method.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\ttol parameter used in policy_evaluation()\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "\n",
    "\t############################\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exposed-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3, epoches=10000):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "    defined at beginning of file\n",
    "    tol: float\n",
    "    Terminate value iteration when\n",
    "    max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    # Used to emulate do-while loop\n",
    "    # Values associated with the new state\n",
    "    new_value_function = np.zeros(nS)\n",
    "    \n",
    "    # Number of iteration to train\n",
    "    current_epoch = 0\n",
    "    \n",
    "    # Used to determine halt point\n",
    "    delta = np.abs(np.sum(new_value_function) - np.sum(value_function))\n",
    "    while current_epoch < epoches or delta >= tol:\n",
    "        for state in range(nS):\n",
    "            # Value associated with a new state S' after taking action a\n",
    "#             print('Calculating actions values')\n",
    "            action_values = []\n",
    "            \n",
    "            # Find best action from current state\n",
    "#             print('\\tIterating over action space')\n",
    "            for action in range(nA):\n",
    "                # Value we got after taking action a\n",
    "                state_value = 0\n",
    "#                 print('\\t\\tComputing value for state:', state, ' and action:', action)\n",
    "                for idx in range(len(P[state][action])):\n",
    "                    prob, next_state, reward, done = P[state][action][idx]\n",
    "                    state_action_value = prob*(reward+gamma*value_function[next_state])\n",
    "                    state_value += state_action_value\n",
    "                \n",
    "                action_values.append(state_value)\n",
    "                best_action = np.argmax(np.asarray(action_values))\n",
    "                new_value_function[state] = action_values[best_action]\n",
    "                policy[state] = best_action\n",
    "        \n",
    "        value_function = new_value_function.copy()\n",
    "        delta = np.abs(np.sum(new_value_function) - np.sum(value_function))\n",
    "        current_epoch += 1\n",
    "    ############################\n",
    "\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "mechanical-liberia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "    print(\"start\")\n",
    "    env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    #env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    # print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    # V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    # render_single(env, p_pi, 100)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_vi, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-three",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
